{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd92125-12bb-4b58-b6a3-e1cfcc6b73c8",
   "metadata": {},
   "source": [
    "# # TRES MODELOS MEJORADOS PARA SEGMENTACI√ìN DE V√âRTEBRAS\n",
    "# \n",
    "# 1. DeepLabV3++ (DeepLabV3+ con Decoder Denso tipo U-Net++)\n",
    "# 2. Hybrid++ (Tu modelo original mejorado con t√©cnicas de DeepLab)\n",
    "# 3. UNet++Lite (U-Net++ optimizada y ligera)\n",
    "# \n",
    "# Todos con segmentaci√≥n de alta calidad estilo U-Net++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf6d80-6423-4287-aa90-dbac01341d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n inicial\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dbe282-e3ee-433b-bc25-135e0707ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    print(f\"‚úì Seed fijado en {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b813164-237c-42a6-b319-0f0bf851349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√ìDULOS COMPARTIDOS\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, 7, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        return x * self.conv(torch.cat([avg_out, max_out], dim=1))\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(self.fc(self.avg_pool(x)) + self.fc(self.max_pool(x)))\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    \"\"\"ASPP mejorado con Channel Attention\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, rates=[6, 12, 18]):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.atrous_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=r, dilation=r, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ) for r in rates\n",
    "        ])\n",
    "        \n",
    "        self.global_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_ch, out_ch, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        total_ch = out_ch * (len(rates) + 2)\n",
    "        self.channel_attention = ChannelAttention(total_ch, reduction=16)\n",
    "        \n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(total_ch, out_ch, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        size = x.shape[2:]\n",
    "        feats = [self.conv1(x)] + [block(x) for block in self.atrous_blocks]\n",
    "        feats.append(F.interpolate(self.global_pool(x), size=size, mode='bilinear', align_corners=True))\n",
    "        feats = torch.cat(feats, dim=1)\n",
    "        feats = self.channel_attention(feats)\n",
    "        return self.project(feats)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Bloque convolucional con BatchNorm y Dropout\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(dropout),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    \"\"\"Attention Gate\"\"\"\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, 1, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, 1, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, 1, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        return x * self.psi(self.relu(g1 + x1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3630ba-59ec-46e1-8402-205e7dd3e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELO 1: DEEPLABV3++ (DeepLabV3+ con Decoder Denso)\n",
    "\n",
    "class DeepLabV3PlusPlus(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepLabV3++ - DeepLabV3+ con Decoder Denso tipo U-Net++\n",
    "    \n",
    "    Mejoras sobre DeepLabV3+ est√°ndar:\n",
    "    - Decoder de 4 niveles (vs 2 en original)\n",
    "    - Skip connections en todos los niveles\n",
    "    - Attention Gates selectivos\n",
    "    - Refinaci√≥n progresiva\n",
    "    \n",
    "    Par√°metros: ~14M\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(3, 64, dropout=0.05)\n",
    "        self.enc2 = ConvBlock(64, 128, dropout=0.1)\n",
    "        self.enc3 = ConvBlock(128, 256, dropout=0.1)\n",
    "        self.enc4 = ConvBlock(256, 512, dropout=0.15)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # ASPP en bottleneck (salida 256 canales)\n",
    "        self.aspp = ASPP(512, 256, rates=[6, 12, 18])\n",
    "        \n",
    "        # Projection para enc4: reducir 512‚Üí256 para match con ASPP\n",
    "        self.enc4_proj = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Decoder DENSO\n",
    "        # Level 4: ASPP(256) + enc4_proj(256) = 512\n",
    "        self.up4 = nn.ConvTranspose2d(256, 256, 2, stride=2)\n",
    "        self.att4 = AttentionGate(256, 256, 128)\n",
    "        self.dec4 = ConvBlock(512, 256, dropout=0.1)\n",
    "        \n",
    "        # Level 3: dec4(256) + enc3(256) = 512\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.enc3_proj = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.att3 = AttentionGate(128, 128, 64)\n",
    "        self.dec3 = ConvBlock(256, 128, dropout=0.1)\n",
    "        \n",
    "        # Level 2: dec3(128) + enc2(128) = 256\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.enc2_proj = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.spatial_att = SpatialAttention()\n",
    "        self.dec2 = ConvBlock(128, 64, dropout=0.05)\n",
    "        \n",
    "        # Level 1: dec2(64) + enc1(64) = 128\n",
    "        self.up1 = nn.ConvTranspose2d(64, 64, 2, stride=2)\n",
    "        self.dec1 = ConvBlock(128, 64, dropout=0.05)\n",
    "        \n",
    "        # Output\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, num_classes, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)              # 64 ch, H√óW\n",
    "        enc2 = self.enc2(self.pool(enc1))  # 128 ch, H/2√óW/2\n",
    "        enc3 = self.enc3(self.pool(enc2))  # 256 ch, H/4√óW/4\n",
    "        enc4 = self.enc4(self.pool(enc3))  # 512 ch, H/8√óW/8\n",
    "        \n",
    "        # ASPP bottleneck\n",
    "        bottleneck = self.aspp(enc4)     # 256 ch, H/8√óW/8\n",
    "        \n",
    "        # Decoder denso con interpolaci√≥n para match de tama√±os\n",
    "        # Level 4\n",
    "        dec4 = self.up4(bottleneck)      # 256 ch, H/4√óW/4\n",
    "        enc4_reduced = self.enc4_proj(enc4)  # 512‚Üí256 ch, H/8√óW/8\n",
    "        # Upsample enc4_reduced para match con dec4\n",
    "        enc4_reduced = F.interpolate(enc4_reduced, size=dec4.shape[2:], \n",
    "                                    mode='bilinear', align_corners=True)\n",
    "        enc4_att = self.att4(dec4, enc4_reduced)\n",
    "        dec4 = torch.cat([dec4, enc4_att], dim=1)  # 256+256=512\n",
    "        dec4 = self.dec4(dec4)           # 256 ch\n",
    "        \n",
    "        # Level 3\n",
    "        dec3 = self.up3(dec4)            # 128 ch, H/2√óW/2\n",
    "        enc3_reduced = self.enc3_proj(enc3)  # 256‚Üí128 ch, H/4√óW/4\n",
    "        # Match size\n",
    "        enc3_reduced = F.interpolate(enc3_reduced, size=dec3.shape[2:], \n",
    "                                    mode='bilinear', align_corners=True)\n",
    "        enc3_att = self.att3(dec3, enc3_reduced)\n",
    "        dec3 = torch.cat([dec3, enc3_att], dim=1)  # 128+128=256\n",
    "        dec3 = self.dec3(dec3)           # 128 ch\n",
    "        \n",
    "        # Level 2\n",
    "        dec2 = self.up2(dec3)            # 64 ch, H√óW\n",
    "        enc2_reduced = self.enc2_proj(enc2)  # 128‚Üí64 ch, H/2√óW/2\n",
    "        # Match size\n",
    "        enc2_reduced = F.interpolate(enc2_reduced, size=dec2.shape[2:], \n",
    "                                    mode='bilinear', align_corners=True)\n",
    "        enc2_att = self.spatial_att(enc2_reduced)\n",
    "        dec2 = torch.cat([dec2, enc2_att], dim=1)  # 64+64=128\n",
    "        dec2 = self.dec2(dec2)           # 64 ch\n",
    "        \n",
    "        # Level 1\n",
    "        dec1 = self.up1(dec2)            # 64 ch, 2H√ó2W\n",
    "        # Match size con enc1\n",
    "        dec1 = F.interpolate(dec1, size=enc1.shape[2:], \n",
    "                           mode='bilinear', align_corners=True)\n",
    "        dec1 = torch.cat([dec1, enc1], dim=1)  # 64+64=128\n",
    "        dec1 = self.dec1(dec1)           # 64 ch\n",
    "        \n",
    "        # Ensure output matches input size\n",
    "        output = self.out(dec1)\n",
    "        if output.shape[2:] != x.shape[2:]:\n",
    "            output = F.interpolate(output, size=x.shape[2:], \n",
    "                                 mode='bilinear', align_corners=True)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2d606-5602-4922-af4f-37204b556245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELO 2: HYBRID++ (Tu modelo con t√©cnicas DeepLab)\n",
    "\n",
    "class HybridPlusPlus(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid++ - Tu modelo Hybrid mejorado con t√©cnicas de DeepLab\n",
    "    \n",
    "    Mejoras:\n",
    "    - ASPP m√°s robusto con Channel Attention\n",
    "    - Dilated convolutions en encoder profundo\n",
    "    - Mejor refinaci√≥n en decoder\n",
    "    - Spatial Attention en m√∫ltiples niveles\n",
    "    \n",
    "    Par√°metros: ~18M (tu modelo optimizado al m√°ximo)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder con dilated conv en niveles profundos\n",
    "        self.enc1 = ConvBlock(3, 64, dropout=0.05)\n",
    "        self.enc2 = ConvBlock(64, 128, dropout=0.1)\n",
    "        self.enc3 = ConvBlock(128, 256, dropout=0.1)\n",
    "        \n",
    "        # Enc4 con dilated convolutions\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, padding=2, dilation=2, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.15),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # ASPP mejorado (salida 256 canales para control de par√°metros)\n",
    "        self.aspp = ASPP(512, 256, rates=[6, 12, 18])\n",
    "        \n",
    "        # Projections para match de canales\n",
    "        self.enc4_proj = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.enc3_proj = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.enc2_proj = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Decoder con attention en todos los niveles\n",
    "        self.up4 = nn.ConvTranspose2d(256, 256, 2, stride=2)\n",
    "        self.att4 = AttentionGate(256, 256, 128)\n",
    "        self.spatial_att4 = SpatialAttention()\n",
    "        self.dec4 = ConvBlock(512, 256, dropout=0.1)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.att3 = AttentionGate(128, 128, 64)\n",
    "        self.spatial_att3 = SpatialAttention()\n",
    "        self.dec3 = ConvBlock(256, 128, dropout=0.1)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.spatial_att2 = SpatialAttention()\n",
    "        self.dec2 = ConvBlock(128, 64, dropout=0.05)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(64, 64, 2, stride=2)\n",
    "        self.dec1 = ConvBlock(128, 64, dropout=0.05)\n",
    "        \n",
    "        # Output refinado\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.Conv2d(32, num_classes, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)              # 64 ch\n",
    "        enc2 = self.enc2(self.pool(enc1))  # 128 ch\n",
    "        enc3 = self.enc3(self.pool(enc2))  # 256 ch\n",
    "        enc4 = self.enc4(self.pool(enc3))  # 512 ch\n",
    "        \n",
    "        # ASPP\n",
    "        bottleneck = self.aspp(enc4)     # 512 ‚Üí 256 ch\n",
    "        \n",
    "        # Decoder con doble attention + size matching\n",
    "        # Level 4: bottleneck(256) + enc4_proj(256)\n",
    "        dec4 = self.up4(bottleneck)\n",
    "        dec4 = F.interpolate(dec4, size=enc4.shape[2:], mode='bilinear', align_corners=True)\n",
    "        enc4_reduced = self.enc4_proj(enc4)  # 512 ‚Üí 256\n",
    "        enc4_att = self.att4(dec4, enc4_reduced)\n",
    "        enc4_att = self.spatial_att4(enc4_att)\n",
    "        dec4 = torch.cat([dec4, enc4_att], dim=1)  # 256+256=512\n",
    "        dec4 = self.dec4(dec4)  # 512 ‚Üí 256\n",
    "        \n",
    "        # Level 3: dec4(256) + enc3_proj(128)\n",
    "        dec3 = self.up3(dec4)\n",
    "        dec3 = F.interpolate(dec3, size=enc3.shape[2:], mode='bilinear', align_corners=True)\n",
    "        enc3_reduced = self.enc3_proj(enc3)  # 256 ‚Üí 128\n",
    "        enc3_att = self.att3(dec3, enc3_reduced)\n",
    "        enc3_att = self.spatial_att3(enc3_att)\n",
    "        dec3 = torch.cat([dec3, enc3_att], dim=1)  # 128+128=256\n",
    "        dec3 = self.dec3(dec3)  # 256 ‚Üí 128\n",
    "        \n",
    "        # Level 2: dec3(128) + enc2_proj(64)\n",
    "        dec2 = self.up2(dec3)\n",
    "        dec2 = F.interpolate(dec2, size=enc2.shape[2:], mode='bilinear', align_corners=True)\n",
    "        enc2_reduced = self.enc2_proj(enc2)  # 128 ‚Üí 64\n",
    "        enc2_att = self.spatial_att2(enc2_reduced)\n",
    "        dec2 = torch.cat([dec2, enc2_att], dim=1)  # 64+64=128\n",
    "        dec2 = self.dec2(dec2)  # 128 ‚Üí 64\n",
    "        \n",
    "        # Level 1: dec2(64) + enc1(64)\n",
    "        dec1 = self.up1(dec2)\n",
    "        dec1 = F.interpolate(dec1, size=enc1.shape[2:], mode='bilinear', align_corners=True)\n",
    "        dec1 = torch.cat([dec1, enc1], dim=1)  # 64+64=128\n",
    "        dec1 = self.dec1(dec1)  # 128 ‚Üí 64\n",
    "        \n",
    "        output = self.out(dec1)\n",
    "        if output.shape[2:] != x.shape[2:]:\n",
    "            output = F.interpolate(output, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660b6d4-9714-46a9-99ac-eb4a34fa7c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELO 3: UNET++ LITE (U-Net++ Optimizada)\n",
    "\n",
    "class UNetPlusPlusLite(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net++ Lite - Versi√≥n optimizada de U-Net++ con nested skip connections\n",
    "    \n",
    "    Caracter√≠sticas:\n",
    "    - Nested skip pathways (caracter√≠stica distintiva de U-Net++)\n",
    "    - Deep supervision (opcional)\n",
    "    - Menos canales que U-Net++ original para eficiencia\n",
    "    \n",
    "    Par√°metros: ~22M (vs ~36M de U-Net++ original)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=3, deep_supervision=False):\n",
    "        super().__init__()\n",
    "        self.deep_supervision = deep_supervision\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(3, 48, dropout=0.05)\n",
    "        self.enc2 = ConvBlock(48, 96, dropout=0.1)\n",
    "        self.enc3 = ConvBlock(96, 192, dropout=0.1)\n",
    "        self.enc4 = ConvBlock(192, 384, dropout=0.15)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = ConvBlock(384, 768, dropout=0.2)\n",
    "        \n",
    "        # Nested skip pathways (n√∫cleo de U-Net++)\n",
    "        self.up1_0 = nn.ConvTranspose2d(768, 384, 2, stride=2)\n",
    "        self.conv1_0 = ConvBlock(768, 384, dropout=0.15)\n",
    "        \n",
    "        self.up2_0 = nn.ConvTranspose2d(384, 192, 2, stride=2)\n",
    "        self.conv2_0 = ConvBlock(384, 192, dropout=0.1)\n",
    "        self.up1_1 = nn.ConvTranspose2d(384, 192, 2, stride=2)\n",
    "        self.conv1_1 = ConvBlock(384, 192, dropout=0.1)\n",
    "        \n",
    "        self.up3_0 = nn.ConvTranspose2d(192, 96, 2, stride=2)\n",
    "        self.conv3_0 = ConvBlock(192, 96, dropout=0.1)\n",
    "        self.up2_1 = nn.ConvTranspose2d(192, 96, 2, stride=2)\n",
    "        self.conv2_1 = ConvBlock(192, 96, dropout=0.1)\n",
    "        self.up1_2 = nn.ConvTranspose2d(192, 96, 2, stride=2)\n",
    "        self.conv1_2 = ConvBlock(192, 96, dropout=0.1)\n",
    "        \n",
    "        self.up4_0 = nn.ConvTranspose2d(96, 48, 2, stride=2)\n",
    "        self.conv4_0 = ConvBlock(96, 48, dropout=0.05)\n",
    "        self.up3_1 = nn.ConvTranspose2d(96, 48, 2, stride=2)\n",
    "        self.conv3_1 = ConvBlock(96, 48, dropout=0.05)\n",
    "        self.up2_2 = nn.ConvTranspose2d(96, 48, 2, stride=2)\n",
    "        self.conv2_2 = ConvBlock(96, 48, dropout=0.05)\n",
    "        self.up1_3 = nn.ConvTranspose2d(96, 48, 2, stride=2)\n",
    "        self.conv1_3 = ConvBlock(96, 48, dropout=0.05)\n",
    "        \n",
    "        # Output\n",
    "        self.out = nn.Conv2d(48, num_classes, 1)\n",
    "        \n",
    "        # Deep supervision outputs (opcional)\n",
    "        if deep_supervision:\n",
    "            self.out1 = nn.Conv2d(96, num_classes, 1)\n",
    "            self.out2 = nn.Conv2d(96, num_classes, 1)\n",
    "            self.out3 = nn.Conv2d(384, num_classes, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1_0 = self.enc1(x)\n",
    "        x2_0 = self.enc2(self.pool(x1_0))\n",
    "        x3_0 = self.enc3(self.pool(x2_0))\n",
    "        x4_0 = self.enc4(self.pool(x3_0))\n",
    "        x5_0 = self.bottleneck(self.pool(x4_0))\n",
    "        \n",
    "        # Nested skip pathways\n",
    "        x1_0_up = self.up1_0(x5_0)\n",
    "        x1_0_cat = torch.cat([x4_0, x1_0_up], dim=1)\n",
    "        x4_1 = self.conv1_0(x1_0_cat)\n",
    "        \n",
    "        x2_0_up = self.up2_0(x4_1)\n",
    "        x2_0_cat = torch.cat([x3_0, x2_0_up], dim=1)\n",
    "        x3_1 = self.conv2_0(x2_0_cat)\n",
    "        \n",
    "        x1_1_up = self.up1_1(x4_1)\n",
    "        x1_1_cat = torch.cat([x3_0, x1_1_up], dim=1)\n",
    "        x3_2 = self.conv1_1(x1_1_cat)\n",
    "        \n",
    "        x3_0_up = self.up3_0(x3_1)\n",
    "        x3_0_cat = torch.cat([x2_0, x3_0_up], dim=1)\n",
    "        x2_1 = self.conv3_0(x3_0_cat)\n",
    "        \n",
    "        x2_1_up = self.up2_1(x3_2)\n",
    "        x2_1_cat = torch.cat([x2_0, x2_1_up], dim=1)\n",
    "        x2_2 = self.conv2_1(x2_1_cat)\n",
    "        \n",
    "        x1_2_up = self.up1_2(x3_2)\n",
    "        x1_2_cat = torch.cat([x2_0, x1_2_up], dim=1)\n",
    "        x2_3 = self.conv1_2(x1_2_cat)\n",
    "        \n",
    "        x4_0_up = self.up4_0(x2_1)\n",
    "        x4_0_cat = torch.cat([x1_0, x4_0_up], dim=1)\n",
    "        x1_1 = self.conv4_0(x4_0_cat)\n",
    "        \n",
    "        x3_1_up = self.up3_1(x2_2)\n",
    "        x3_1_cat = torch.cat([x1_0, x3_1_up], dim=1)\n",
    "        x1_2 = self.conv3_1(x3_1_cat)\n",
    "        \n",
    "        x2_2_up = self.up2_2(x2_3)\n",
    "        x2_2_cat = torch.cat([x1_0, x2_2_up], dim=1)\n",
    "        x1_3 = self.conv2_2(x2_2_cat)\n",
    "        \n",
    "        x1_3_up = self.up1_3(x2_3)\n",
    "        x1_3_cat = torch.cat([x1_0, x1_3_up], dim=1)\n",
    "        x1_4 = self.conv1_3(x1_3_cat)\n",
    "        \n",
    "        # Output\n",
    "        output = self.out(x1_4)\n",
    "        \n",
    "        if self.deep_supervision:\n",
    "            return [output, self.out1(x1_3), self.out2(x1_2), self.out3(x4_1)]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f666924e-b0a0-4486-b617-51c3db3d8f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET\n",
    "\n",
    "class VertebrasDataset(Dataset):\n",
    "    def __init__(self, base_path, json_filename='coco_anotaciones_actualizadas_23sep.json',\n",
    "                 target_size=(256, 256)):\n",
    "        \n",
    "        self.base_path = Path(base_path)\n",
    "        self.json_path = self.base_path / \"Anotaciones v√©rtebras\" / json_filename\n",
    "        self.radiografias_path = self.base_path / \"Radiograf√≠as\"\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        with open(self.json_path, 'r', encoding='utf-8') as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        \n",
    "        self.class_names = ['Background', 'T1', 'V']\n",
    "        self.name_to_class = {'F': 0, 'background': 0, 'T1': 1, 'V': 2}\n",
    "        self.samples = self._preparar_samples()\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.399637, 0.400040, 0.392532],\n",
    "                std=[0.212403, 0.211738, 0.207753]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def _preparar_samples(self):\n",
    "        samples = []\n",
    "        anns_por_imagen = defaultdict(list)\n",
    "        \n",
    "        for ann in self.coco_data['annotations']:\n",
    "            img_id = ann.get('image_id')\n",
    "            if img_id is not None:\n",
    "                anns_por_imagen[img_id].append(ann)\n",
    "        \n",
    "        for img_info in self.coco_data['images']:\n",
    "            img_id = img_info.get('id')\n",
    "            if img_id not in anns_por_imagen:\n",
    "                continue\n",
    "            \n",
    "            file_name = img_info.get('file_name') or img_info.get('toras_path', '') or ''\n",
    "            if file_name.startswith('/'):\n",
    "                file_name = file_name[1:]\n",
    "            \n",
    "            img_path = self.radiografias_path / file_name\n",
    "            if not img_path.exists():\n",
    "                img_path = self.radiografias_path / Path(file_name).name\n",
    "            \n",
    "            if img_path.exists():\n",
    "                samples.append({\n",
    "                    'image_id': img_id,\n",
    "                    'image_path': str(img_path),\n",
    "                    'annotations': anns_por_imagen[img_id]\n",
    "                })\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _parsear_segmentacion(self, segmentation):\n",
    "        poligonos = []\n",
    "        if not segmentation or not isinstance(segmentation, list):\n",
    "            return poligonos\n",
    "        \n",
    "        for seg_item in segmentation:\n",
    "            if isinstance(seg_item, list) and seg_item:\n",
    "                if isinstance(seg_item[0], (int, float)) and len(seg_item) >= 6:\n",
    "                    coords = np.array(seg_item).reshape(-1, 2)\n",
    "                    if coords.shape[0] >= 3:\n",
    "                        poligonos.append(coords.astype(np.int32))\n",
    "        return poligonos\n",
    "    \n",
    "    def _crear_mascara(self, annotations, orig_height, orig_width):\n",
    "        mask = np.zeros((orig_height, orig_width), dtype=np.uint8)\n",
    "        \n",
    "        for ann in annotations:\n",
    "            name = ann.get('name', '').strip()\n",
    "            if name not in self.name_to_class:\n",
    "                continue\n",
    "            \n",
    "            class_id = self.name_to_class[name]\n",
    "            poligonos = self._parsear_segmentacion(ann.get('segmentation'))\n",
    "            \n",
    "            for poly in poligonos:\n",
    "                cv2.fillPoly(mask, [poly], class_id)\n",
    "            \n",
    "            if not poligonos and 'bbox' in ann:\n",
    "                bbox = ann['bbox']\n",
    "                x, y, w, h = [int(v) for v in bbox]\n",
    "                if x >= 0 and y >= 0 and w > 0 and h > 0:\n",
    "                    x2, y2 = min(x+w, orig_width), min(y+h, orig_height)\n",
    "                    mask[y:y2, x:x2] = class_id\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        image = Image.open(sample['image_path']).convert('RGB')\n",
    "        orig_width, orig_height = image.size\n",
    "        mask = self._crear_mascara(sample['annotations'], orig_height, orig_width)\n",
    "        \n",
    "        image = image.resize(self.target_size, Image.BILINEAR)\n",
    "        mask = cv2.resize(mask, self.target_size, interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        image = self.transform(image)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab494b7-e162-43cb-b819-488de9f7643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS Y M√âTRICAS\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        target_one_hot = F.one_hot(target, num_classes=pred.shape[1]).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        intersection = (pred * target_one_hot).sum(dim=(2, 3))\n",
    "        union = pred.sum(dim=(2, 3)) + target_one_hot.sum(dim=(2, 3))\n",
    "        \n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1.0 - dice.mean()\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        ce_loss = F.cross_entropy(pred, target, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        return (self.alpha * (1 - pt) ** self.gamma * ce_loss).mean()\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, focal_weight=0.3, dice_weight=0.7):\n",
    "        super().__init__()\n",
    "        self.focal_weight = focal_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.focal_loss = FocalLoss()\n",
    "        self.dice_loss = DiceLoss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        return self.focal_weight * self.focal_loss(pred, target) + self.dice_weight * self.dice_loss(pred, target)\n",
    "\n",
    "\n",
    "def calcular_metricas_detalladas(pred, target, num_classes, class_names):\n",
    "    pred_classes = torch.argmax(pred, dim=1)\n",
    "    metricas = {}\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        pred_c = (pred_classes == c)\n",
    "        target_c = (target == c)\n",
    "        intersection = (pred_c & target_c).sum().float()\n",
    "        union = (pred_c | target_c).sum().float()\n",
    "        \n",
    "        if union > 0:\n",
    "            iou = (intersection / union).item()\n",
    "            dice = (2 * intersection / (pred_c.sum() + target_c.sum())).item()\n",
    "        else:\n",
    "            iou = 0.0\n",
    "            dice = 0.0\n",
    "        \n",
    "        class_name = class_names[c] if c < len(class_names) else f'class_{c}'\n",
    "        metricas[f'iou_{class_name}'] = iou\n",
    "        metricas[f'dice_{class_name}'] = dice\n",
    "    \n",
    "    ious_sin_bg = [metricas[f'iou_{class_names[c]}'] for c in range(1, num_classes)]\n",
    "    dices_sin_bg = [metricas[f'dice_{class_names[c]}'] for c in range(1, num_classes)]\n",
    "    \n",
    "    metricas['mean_iou'] = np.mean(ious_sin_bg) if ious_sin_bg else 0.0\n",
    "    metricas['mean_dice'] = np.mean(dices_sin_bg) if dices_sin_bg else 0.0\n",
    "    metricas['accuracy'] = (pred_classes == target).float().mean().item()\n",
    "    \n",
    "    return metricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2ad39b-89f4-43e2-8ca2-8a0e3db16f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONES DE ENTRENAMIENTO\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, num_classes, class_names):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_metrics = defaultdict(float)\n",
    "    \n",
    "    for images, masks in dataloader:\n",
    "        if images.size(0) == 1:\n",
    "            continue\n",
    "            \n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        metricas = calcular_metricas_detalladas(outputs, masks, num_classes, class_names)\n",
    "        for k, v in metricas.items():\n",
    "            total_metrics[k] += v\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_metrics = {k: v / len(dataloader) for k, v in total_metrics.items()}\n",
    "    return avg_loss, avg_metrics\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device, num_classes, class_names):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_metrics = defaultdict(float)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            metricas = calcular_metricas_detalladas(outputs, masks, num_classes, class_names)\n",
    "            for k, v in metricas.items():\n",
    "                total_metrics[k] += v\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_metrics = {k: v / len(dataloader) for k, v in total_metrics.items()}\n",
    "    return avg_loss, avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70092dca-0c9e-4af2-a11e-a3bc11c35cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZACIONES\n",
    "\n",
    "def visualizar_comparacion_3modelos(models, model_names, dataset, device, class_names, num_samples=3, seed=42):\n",
    "    \"\"\"Compara los 3 modelos lado a lado\"\"\"\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    \n",
    "    colors = {0: [0, 0, 0], 1: [0, 255, 0], 2: [0, 0, 255]}\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 5, figsize=(22, num_samples * 4))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "    \n",
    "    mean = torch.tensor([0.399637, 0.400040, 0.392532]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.212403, 0.211738, 0.207753]).view(3, 1, 1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(indices):\n",
    "            image, mask = dataset[idx]\n",
    "            image_input = image.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Desnormalizar imagen\n",
    "            image_denorm = image * std + mean\n",
    "            img_np = image_denorm.permute(1, 2, 0).numpy()\n",
    "            img_np = np.clip(img_np, 0, 1)\n",
    "            \n",
    "            mask_np = mask.numpy()\n",
    "            mask_colored = np.zeros((*mask_np.shape, 3), dtype=np.uint8)\n",
    "            for class_id, color in colors.items():\n",
    "                mask_colored[mask_np == class_id] = color\n",
    "            \n",
    "            # Original\n",
    "            axes[i, 0].imshow(img_np)\n",
    "            axes[i, 0].set_title('üñºÔ∏è Original', fontsize=11, fontweight='bold')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            # Ground Truth\n",
    "            axes[i, 1].imshow(mask_colored)\n",
    "            axes[i, 1].set_title('‚úì Ground Truth', fontsize=11, fontweight='bold')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            # Predicciones de los 3 modelos\n",
    "            for j, (model, name) in enumerate(zip(models, model_names)):\n",
    "                pred = model(image_input).cpu().squeeze(0)\n",
    "                pred_classes = torch.argmax(pred, dim=0).numpy()\n",
    "                \n",
    "                pred_colored = np.zeros((*pred_classes.shape, 3), dtype=np.uint8)\n",
    "                for class_id, color in colors.items():\n",
    "                    pred_colored[pred_classes == class_id] = color\n",
    "                \n",
    "                metricas = calcular_metricas_detalladas(\n",
    "                    pred.unsqueeze(0), mask.unsqueeze(0), len(colors), class_names\n",
    "                )\n",
    "                \n",
    "                axes[i, j+2].imshow(pred_colored)\n",
    "                axes[i, j+2].set_title(f'{name}\\nIoU: {metricas[\"mean_iou\"]:.3f}', \n",
    "                                      fontsize=10, fontweight='bold')\n",
    "                axes[i, j+2].axis('off')\n",
    "    \n",
    "    legend_elements = [\n",
    "        plt.Rectangle((0, 0), 1, 1, fc=np.array(colors[i])/255.0, \n",
    "                     edgecolor='black', linewidth=2, label=class_names[i]) \n",
    "        for i in range(1, len(class_names))\n",
    "    ]\n",
    "    fig.legend(handles=legend_elements, loc='upper center', ncol=len(class_names)-1, \n",
    "              fontsize=12, frameon=True, fancybox=True, shadow=True)\n",
    "    \n",
    "    plt.suptitle('üîç COMPARACI√ìN: 3 MODELOS', fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    \n",
    "    filename = f'comparison_3models_seed{seed}.png'\n",
    "    plt.savefig(filename, dpi=200, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "    print(f\"üì∏ Comparaci√≥n guardada: {filename}\")\n",
    "\n",
    "\n",
    "def crear_tabla_final_comparativa(resultados):\n",
    "    \"\"\"Tabla comparativa final de los 3 modelos\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    \n",
    "    nombres = [r['name'] for r in resultados]\n",
    "    ious = [r['iou'] for r in resultados]\n",
    "    params = [r['params'] / 1e6 for r in resultados]\n",
    "    \n",
    "    # Gr√°fico 1: IoU\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    bars = ax1.barh(nombres, ious, color=colors, edgecolor='black', linewidth=2)\n",
    "    ax1.set_xlabel('IoU Promedio', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('üèÜ Comparaci√≥n de IoU', fontsize=15, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "    ax1.set_xlim([0, max(ious) * 1.1])\n",
    "    \n",
    "    for bar, iou in zip(bars, ious):\n",
    "        ax1.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "                f'{iou:.4f}', va='center', ha='left', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Gr√°fico 2: Eficiencia\n",
    "    scatter = ax2.scatter(params, ious, s=500, c=colors, edgecolor='black', \n",
    "                         linewidth=3, alpha=0.8)\n",
    "    \n",
    "    for i, nombre in enumerate(nombres):\n",
    "        ax2.annotate(nombre, (params[i], ious[i]), \n",
    "                    xytext=(10, 10), textcoords='offset points',\n",
    "                    fontsize=11, fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.4))\n",
    "    \n",
    "    ax2.set_xlabel('Par√°metros (Millones)', fontsize=13, fontweight='bold')\n",
    "    ax2.set_ylabel('IoU Promedio', fontsize=13, fontweight='bold')\n",
    "    ax2.set_title('‚öñÔ∏è Eficiencia: IoU vs Tama√±o', fontsize=15, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('üìä AN√ÅLISIS COMPARATIVO - 3 MODELOS FINALES', \n",
    "                fontsize=17, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    \n",
    "    filename = 'final_comparison_3models.png'\n",
    "    plt.savefig(filename, dpi=200, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "    print(f\"üìä Tabla final guardada: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaddae4-c555-4d76-b351-5b8f795c3187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN - ENTRENAR Y COMPARAR 3 MODELOS\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üî¨ COMPARACI√ìN: 3 MODELOS OPTIMIZADOS\")\n",
    "    print(\"   1. DeepLabV3++ (DeepLab con decoder denso)\")\n",
    "    print(\"   2. Hybrid++ (Tu modelo optimizado)\")\n",
    "    print(\"   3. U-Net++ Lite (U-Net++ eficiente)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # CONFIGURACI√ìN\n",
    "    SEED = 42\n",
    "    BASE_PATH = r\"C:\\Users\\User\\Documents\\Proyectofinal\"\n",
    "    BATCH_SIZE = 8\n",
    "    MAX_EPOCHS = 100\n",
    "    LEARNING_RATE = 0.0001\n",
    "    IMAGE_SIZE = 256\n",
    "    NUM_CLASSES = 3\n",
    "    \n",
    "    # ¬øQu√© modelo entrenar? (puedes elegir uno o todos)\n",
    "    TRAIN_MODEL = input(\"\\n¬øQu√© modelo entrenar? (1=DeepLabV3++, 2=Hybrid++, 3=UNet++Lite, 4=Todos): \").strip()\n",
    "    \n",
    "    set_seed(SEED)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nüíª Dispositivo: {device}\")\n",
    "    \n",
    "    # DATASET\n",
    "    print(\"\\nüì¶ Cargando dataset...\")\n",
    "    full_dataset = VertebrasDataset(BASE_PATH, target_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "    class_names = full_dataset.class_names\n",
    "    \n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    \n",
    "    generator = torch.Generator().manual_seed(SEED)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Total: {len(full_dataset)} | Train: {len(train_dataset)} | Val: {len(val_dataset)}\")\n",
    "    \n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = SEED + worker_id\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "    \n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=0, worker_init_fn=seed_worker, generator=g, drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "        num_workers=0, worker_init_fn=seed_worker, drop_last=False\n",
    "    )\n",
    "    \n",
    "    # DEFINIR MODELOS\n",
    "    modelos_config = {\n",
    "        '1': ('DeepLabV3++', DeepLabV3PlusPlus(NUM_CLASSES)),\n",
    "        '2': ('Hybrid++', HybridPlusPlus(NUM_CLASSES)),\n",
    "        '3': ('U-Net++ Lite', UNetPlusPlusLite(NUM_CLASSES, deep_supervision=False))\n",
    "    }\n",
    "    \n",
    "    if TRAIN_MODEL == '4':\n",
    "        modelos_a_entrenar = ['1', '2', '3']\n",
    "    else:\n",
    "        modelos_a_entrenar = [TRAIN_MODEL]\n",
    "    \n",
    "    resultados_finales = []\n",
    "    \n",
    "    # ENTRENAR MODELOS\n",
    "    for modelo_id in modelos_a_entrenar:\n",
    "        if modelo_id not in modelos_config:\n",
    "            continue\n",
    "        \n",
    "        nombre_modelo, modelo = modelos_config[modelo_id]\n",
    "        modelo = modelo.to(device)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"üèóÔ∏è ENTRENANDO: {nombre_modelo}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in modelo.parameters())\n",
    "        print(f\"‚úì Par√°metros: {total_params/1e6:.2f}M ({total_params:,})\")\n",
    "        \n",
    "        criterion = CombinedLoss(focal_weight=0.3, dice_weight=0.7)\n",
    "        optimizer = optim.AdamW(modelo.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=7)\n",
    "        \n",
    "        best_iou = 0\n",
    "        best_dice = 0\n",
    "        best_epoch = 0\n",
    "        patience_counter = 0\n",
    "        patience = 15\n",
    "        \n",
    "        print(f\"\\nüöÄ Iniciando entrenamiento...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(MAX_EPOCHS):\n",
    "            train_loss, train_metrics = train_epoch(\n",
    "                modelo, train_loader, criterion, optimizer, device, NUM_CLASSES, class_names\n",
    "            )\n",
    "            \n",
    "            val_loss, val_metrics = validate(\n",
    "                modelo, val_loader, criterion, device, NUM_CLASSES, class_names\n",
    "            )\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoca {epoch+1}/{MAX_EPOCHS} | Val IoU: {val_metrics['mean_iou']:.4f} | \"\n",
    "                      f\"Dice: {val_metrics['mean_dice']:.4f}\")\n",
    "            \n",
    "            if val_metrics['mean_iou'] > best_iou:\n",
    "                best_iou = val_metrics['mean_iou']\n",
    "                best_dice = val_metrics['mean_dice']\n",
    "                best_epoch = epoch + 1\n",
    "                patience_counter = 0\n",
    "                \n",
    "                best_metrics_per_class = {\n",
    "                    class_names[c]: {\n",
    "                        'iou': val_metrics[f'iou_{class_names[c]}'],\n",
    "                        'dice': val_metrics[f'dice_{class_names[c]}']\n",
    "                    }\n",
    "                    for c in range(1, NUM_CLASSES)\n",
    "                }\n",
    "                \n",
    "                torch.save(modelo.state_dict(), f'{nombre_modelo.replace(\" \", \"_\").replace(\"+\", \"p\")}_best.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            scheduler.step(val_metrics['mean_iou'])\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"‚èπÔ∏è  Early stopping en epoca {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\n‚úÖ {nombre_modelo} completado!\")\n",
    "        print(f\"   Mejor IoU: {best_iou:.4f} | Dice: {best_dice:.4f} (Epoca {best_epoch})\")\n",
    "        print(f\"   Tiempo: {training_time/60:.2f} min\")\n",
    "        \n",
    "        resultados_finales.append({\n",
    "            'name': nombre_modelo,\n",
    "            'model': modelo,\n",
    "            'iou': best_iou,\n",
    "            'dice': best_dice,\n",
    "            'params': total_params,\n",
    "            'time': training_time,\n",
    "            'epoch': best_epoch,\n",
    "            'metrics_per_class': best_metrics_per_class\n",
    "        })\n",
    "    \n",
    "    # COMPARACI√ìN FINAL\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä RESULTADOS FINALES - COMPARACI√ìN\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n{'Modelo':<20} {'IoU':<10} {'Dice':<10} {'Params':<12} {'Tiempo':<12}\")\n",
    "    print(f\"{'-'*64}\")\n",
    "    for r in resultados_finales:\n",
    "        print(f\"{r['name']:<20} {r['iou']:<10.4f} {r['dice']:<10.4f} \"\n",
    "              f\"{r['params']/1e6:>6.2f}M     {r['time']/60:>6.2f} min\")\n",
    "    \n",
    "    # Mejor modelo\n",
    "    mejor = max(resultados_finales, key=lambda x: x['iou'])\n",
    "    print(f\"\\nüèÜ GANADOR: {mejor['name']} con IoU {mejor['iou']:.4f}\")\n",
    "    \n",
    "    # M√°s eficiente\n",
    "    eficiencias = [(r['name'], r['iou'] / (r['params'] / 1e6) * 100) for r in resultados_finales]\n",
    "    mas_eficiente = max(eficiencias, key=lambda x: x[1])\n",
    "    print(f\"‚ö° M√ÅS EFICIENTE: {mas_eficiente[0]} ({mas_eficiente[1]:.2f} eficiencia)\")\n",
    "    \n",
    "    # VISUALIZACIONES COMPARATIVAS\n",
    "    if len(resultados_finales) > 1:\n",
    "        print(\"\\nüì∏ Generando visualizaciones comparativas...\")\n",
    "        \n",
    "        modelos = [r['model'] for r in resultados_finales]\n",
    "        nombres = [r['name'] for r in resultados_finales]\n",
    "        \n",
    "        visualizar_comparacion_3modelos(modelos, nombres, val_dataset, device, \n",
    "                                       class_names, num_samples=4, seed=SEED)\n",
    "        \n",
    "        crear_tabla_final_comparativa(resultados_finales)\n",
    "    \n",
    "    # RESUMEN DETALLADO\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üìã AN√ÅLISIS DETALLADO POR MODELO\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for r in resultados_finales:\n",
    "        print(f\"\\nüî¨ {r['name']}:\")\n",
    "        print(f\"   IoU: {r['iou']:.4f} | Dice: {r['dice']:.4f}\")\n",
    "        print(f\"   Par√°metros: {r['params']/1e6:.2f}M\")\n",
    "        print(f\"   Tiempo: {r['time']/60:.2f} min | Epoca: {r['epoch']}\")\n",
    "        print(f\"   M√©tricas por clase:\")\n",
    "        for vertebra in class_names[1:]:\n",
    "            iou = r['metrics_per_class'][vertebra]['iou']\n",
    "            dice = r['metrics_per_class'][vertebra]['dice']\n",
    "            print(f\"      {vertebra}: IoU={iou:.4f} | Dice={dice:.4f}\")\n",
    "    \n",
    "    # RECOMENDACIONES\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üí° RECOMENDACIONES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nüéØ CU√ÅNDO USAR CADA MODELO:\")\n",
    "    print(f\"   ‚Ä¢ DeepLabV3++: Balance entre precisi√≥n y eficiencia\")\n",
    "    print(f\"   ‚Ä¢ Hybrid++: M√°xima precisi√≥n con atenci√≥n multi-nivel\")\n",
    "    print(f\"   ‚Ä¢ U-Net++ Lite: Segmentaci√≥n tipo U-Net++ pero m√°s ligera\")\n",
    "    \n",
    "    print(f\"\\nüìÅ ARCHIVOS GENERADOS:\")\n",
    "    for r in resultados_finales:\n",
    "        filename = f'{r[\"name\"].replace(\" \", \"_\").replace(\"+\", \"p\")}_best.pth'\n",
    "        print(f\"   ‚úì {filename}\")\n",
    "    if len(resultados_finales) > 1:\n",
    "        print(f\"   ‚úì comparison_3models_seed{SEED}.png\")\n",
    "        print(f\"   ‚úì final_comparison_3models.png\")\n",
    "    \n",
    "    return resultados_finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eb867e-c910-4ce2-a517-a19c01f0b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJECUCI√ìN PRINCIPAL\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"\\n\" + \"üöÄ\"*40)\n",
    "        print(\"SISTEMA DE COMPARACI√ìN: 3 MODELOS OPTIMIZADOS\")\n",
    "        print(\"üöÄ\"*40 + \"\\n\")\n",
    "        \n",
    "        resultados = main()\n",
    "        \n",
    "        print(\"\\n\" + \"üéâ\"*40)\n",
    "        print(\"AN√ÅLISIS COMPLETADO\")\n",
    "        print(\"üéâ\"*40 + \"\\n\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n‚ö†Ô∏è  Proceso interrumpido\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n‚ùå Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"\\nüßπ Memoria GPU liberada\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
